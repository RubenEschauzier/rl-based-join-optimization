{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c01623",
   "metadata": {},
   "source": [
    "# This Script transforms a .ttl file with URIS containing ids to a .txt file in the \"G-Care\" format\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fa6714f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:39:41.007409Z",
     "start_time": "2025-09-02T11:39:40.996411Z"
    }
   },
   "source": [
    "# From: https://github.com/DE-TUM/GNCE\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "id": "d8a2ad1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:39:41.022409Z",
     "start_time": "2025-09-02T11:39:41.011409Z"
    }
   },
   "source": [
    "# Sets of entities and predicates\n",
    "vertices = set()\n",
    "predicates = set()"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "id": "8f5ebaa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:39:41.630996Z",
     "start_time": "2025-09-02T11:39:41.617995Z"
    }
   },
   "source": [
    "# For which dataset to load the turtle or nt file\n",
    "dataset = \"watdiv\"\n",
    "# Under which name to save the resulting g-care graph\n",
    "gcare_graph_savename = \"watdiv\"\n",
    "if dataset == \"watdiv\":\n",
    "    delimiter = \"\\t\"\n",
    "else:\n",
    "    delimiter = \" \""
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "id": "052ada4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:44:55.474813Z",
     "start_time": "2025-09-02T11:44:55.458807Z"
    }
   },
   "source": [
    "from src.utils.validation_utils.utils_prepare_gcare_inputs import dataset_to_g_care\n",
    "\n",
    "# Declare which URI corresponds to rdf:type\n",
    "rdf_type_uri = dataset_to_g_care[dataset]\n",
    "\n",
    "if rdf_type_uri is None:\n",
    "    raise AssertionError(\"rdf type uri missing !\")\n",
    "\n",
    "# dataset_location = r\"C:\\Users\\ruben\\Downloads\\datasets_used_gcne\\yago\\graph\\yago.nt\"\n",
    "# output_location = r\"C:\\Users\\ruben\\Downloads\\datasets_used_gcne\\{}\\{}.txt\".format(dataset, gcare_graph_savename)\n",
    "# output_id_to_id = r\"C:\\Users\\ruben\\Downloads\\datasets_used_gcne\\{}\\id_to_id_{}.json\".format(dataset, gcare_graph_savename)\n",
    "# output_id_to_id_predicate = r\"C:\\Users\\ruben\\Downloads\\datasets_used_gcne\\{}\\id_to_id_predicate_{}.json\".format(dataset, gcare_graph_savename)\n",
    "\n",
    "dataset_location = r\"C:\\Users\\ruben\\projects\\rl-based-join-optimization\\large_data\\{}.nt\".format(dataset)\n",
    "output_location = r\"C:\\Users\\ruben\\projects\\rl-based-join-optimization\\data\\benchmark_g_care_format\\{}\\{}.txt\".format(dataset, gcare_graph_savename)\n",
    "output_id_to_id = r\"C:\\Users\\ruben\\projects\\rl-based-join-optimization\\data\\benchmark_g_care_format\\{}\\id_to_id_{}.json\".format(dataset,\n",
    "                                                                                            gcare_graph_savename)\n",
    "output_id_to_id_predicate = r\"C:\\Users\\ruben\\projects\\rl-based-join-optimization\\data\\benchmark_g_care_format\\{}\\id_to_id_predicate_{}.json\".format(dataset,gcare_graph_savename)\n",
    "print(output_location)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruben\\projects\\rl-based-join-optimization\\data\\benchmark_g_care_format\\watdiv\\watdiv.txt\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "id": "d3df23cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:39:42.864343Z",
     "start_time": "2025-09-02T11:39:42.850338Z"
    }
   },
   "source": [
    "# OPTIONAL: define a set of entities to exclude from the graph. Those wont be stored as vertices,\n",
    "# and related edges are also not stored\n",
    "excluded_entities = set()\n",
    "\n",
    "# excluded_query_type = 'star'\n",
    "# In this case, we open the inductive test set from the graph\n",
    "# Specifically, we are adding all the objects only, as the subjects are always variables,\n",
    "# and the predicates are assumed to be known\n",
    "\n",
    "\n",
    "# with open(f\"/home/tim/Datasets/{dataset}/{excluded_query_type}/disjoint_test.json\") as f:\n",
    "#     test_data = json.load(f)\n",
    "    \n",
    "# for query in test_data:\n",
    "#     excluded_entities.update([a[2] for a in query[\"triples\"] if not \"?\" in a[2]])"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "id": "151c3656",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:39:43.426343Z",
     "start_time": "2025-09-02T11:39:43.411735Z"
    }
   },
   "source": [
    "len(excluded_entities)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "id": "2bb16958",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:39:49.081076Z",
     "start_time": "2025-09-02T11:39:43.988488Z"
    }
   },
   "source": [
    "# Add all entities and predicates from the .ttl file\n",
    "n_excluded_entites = set()\n",
    "l = 0\n",
    "ttl_file = open(dataset_location, \"r\")\n",
    "for line in tqdm(ttl_file):\n",
    "    if delimiter == \"\\t\":\n",
    "        line = line.replace(\".\", \"\").strip()\n",
    "    atoms = line.split(delimiter)\n",
    "    if delimiter != \"\\t\":\n",
    "        atoms = atoms[:-1]\n",
    "    #if not atoms == []:\n",
    "    if True:\n",
    "        l += 1\n",
    "        if not atoms[2] in excluded_entities:\n",
    "            vertices.add(atoms[0])\n",
    "        if not atoms[2] in excluded_entities:\n",
    "            vertices.add(atoms[2])\n",
    "        else:\n",
    "            n_excluded_entites.update([atoms[2]])\n",
    "        predicates.add(atoms[1])\n",
    "print('Finished collecting vertices and predicates')\n",
    "print(f'Excluded {len(n_excluded_entites)} entities')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3327460it [00:05, 655321.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished collecting vertices and predicates\n",
      "Excluded 0 entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "id": "36e5b2c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:39:49.783118Z",
     "start_time": "2025-09-02T11:39:49.768113Z"
    }
   },
   "source": [
    "assert len(excluded_entities)==len(n_excluded_entites)"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "id": "aeb38a14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:39:50.475025Z",
     "start_time": "2025-09-02T11:39:50.461991Z"
    }
   },
   "source": [
    "# We need to map the URL ids to entity and predicate ids\n",
    "id_to_id_mapping = {}\n",
    "id_to_id_mapping_predicate = {}"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "id": "990b6d4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:39:51.363008Z",
     "start_time": "2025-09-02T11:39:50.896013Z"
    }
   },
   "source": [
    "# Creating Vertex Dict and save entity mappings\n",
    "vertex_dict = {}\n",
    "vid = 0\n",
    "for vertex in tqdm(vertices):\n",
    "    dvid = vertex.split(\"/\")[-1].replace(\">\", \"\")\n",
    "    #vertex_dict[vertex] = [dvid]\n",
    "    vertex_dict[vertex] = [vid]\n",
    "    id_to_id_mapping[vertex] = vid\n",
    "    vid += 1"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321342/321342 [00:00<00:00, 805368.08it/s]\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "id": "5be799a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:39:51.846051Z",
     "start_time": "2025-09-02T11:39:51.832052Z"
    }
   },
   "source": [
    "# Saving Mappings for Predicates\n",
    "pid = 0\n",
    "for p in predicates:\n",
    "    id_to_id_mapping_predicate[p] = pid\n",
    "    pid += 1"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "id": "94632e02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:39:55.982812Z",
     "start_time": "2025-09-02T11:39:52.317346Z"
    }
   },
   "source": [
    "# Add Vertex types\n",
    "ttl_file = open(dataset_location, \"r\")\n",
    "for line in tqdm(ttl_file):\n",
    "    if delimiter == \"\\t\":\n",
    "        line = line.replace(\".\", \"\").strip()\n",
    "    atoms = line.split(delimiter)\n",
    "    if delimiter != \"\\t\":\n",
    "        atoms = atoms[:-1]\n",
    "    if not (atoms[0] in excluded_entities) and not (atoms[2] in excluded_entities):\n",
    "        # If triple has predicate rdf:type\n",
    "        if atoms[1] == rdf_type_uri:\n",
    "            vertex_dict[atoms[0]] += vertex_dict[atoms[2]]\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3327460it [00:03, 910703.25it/s]\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "id": "f2a3fb43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:39:56.607038Z",
     "start_time": "2025-09-02T11:39:56.500016Z"
    }
   },
   "source": [
    "# Add Default Label if entity has no types:\n",
    "for v in vertex_dict:\n",
    "    if len(vertex_dict[v]) == 1:\n",
    "        vertex_dict[v].append(0)"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "id": "2904ad4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:44:14.173521Z",
     "start_time": "2025-09-02T11:44:05.637471Z"
    }
   },
   "source": [
    "# Creating Edge List\n",
    "n_skipped_edges = 0\n",
    "edge_list = []\n",
    "ttl_file = open(dataset_location, \"r\")\n",
    "\n",
    "for tp in tqdm(ttl_file):\n",
    "    if delimiter == \"\\t\":\n",
    "        tp = tp.replace(\".\", \"\").strip()\n",
    "    tp = tp.split(delimiter)\n",
    "    if delimiter != \"\\t\":\n",
    "        tp = tp[:-1]\n",
    "    if not (tp[0] in excluded_entities) and not (tp[2] in excluded_entities):\n",
    "    #edge_label = tp[1].split(\"/\")[-1].replace(\">\", \"\") if not \"?\" in tp[1] else -1\n",
    "        edge_list.append([vertex_dict[tp[0]][0], vertex_dict[tp[2]][0], id_to_id_mapping_predicate[tp[1]]])\n",
    "    else:\n",
    "        n_skipped_edges +=1\n",
    "print('Finished creating edge list')\n",
    "print(f'Dropped a total of {n_skipped_edges} edges')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key\n",
      "<http://dbuwaterlooca/~galuc/wsdbm/Review29462>\n",
      "Val\n",
      "[0, 0]\n",
      "Key\n",
      "\"7392521\"\n",
      "Val\n",
      "[1, 0]\n",
      "Key\n",
      "\"5258962\"\n",
      "Val\n",
      "[2, 0]\n",
      "Key\n",
      "<http://dbuwaterlooca/~galuc/wsdbm/Review40042>\n",
      "Val\n",
      "[3, 0]\n",
      "Key\n",
      "\"94864615\"\n",
      "Val\n",
      "[4, 0]\n",
      "Key\n",
      "<http://dbuwaterlooca/~galuc/wsdbm/Review33975>\n",
      "Val\n",
      "[5, 0]\n",
      "Key\n",
      "<http://dbuwaterlooca/~galuc/wsdbm/Purchase18750>\n",
      "Val\n",
      "[6, 0]\n",
      "Key\n",
      "\"regulator's unpinned violations\"\n",
      "Val\n",
      "[7, 0]\n",
      "Key\n",
      "<http://dbuwaterlooca/~galuc/wsdbm/Review18800>\n",
      "Val\n",
      "[8, 0]\n",
      "Key\n",
      "<http://dbuwaterlooca/~galuc/wsdbm/Purchase27228>\n",
      "Val\n",
      "[9, 0]\n",
      "Key\n",
      "\"mincemeat succored shaman's tigress gotten inhumanity decoded\"\n",
      "Val\n",
      "[10, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3327460it [00:08, 390818.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating edge list\n",
      "Dropped a total of 0 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "id": "fbc7255e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:44:25.409961Z",
     "start_time": "2025-09-02T11:44:19.654802Z"
    }
   },
   "source": [
    "# Writing the Data File\n",
    "with open(output_location, \"w\") as f:\n",
    "    f.write(\"t # 1\")\n",
    "    f.write(\"\\n\")\n",
    "    for v in tqdm(vertex_dict):\n",
    "        f.write(\"v\")\n",
    "        for p in vertex_dict[v]:\n",
    "            f.write(\" \")\n",
    "            f.write(str(p) + \"\")\n",
    "        #f.write(\"v \" + str(vertex_dict[v][0]) + \" \")\n",
    "        f.write(\"\\n\")\n",
    "    for e in tqdm(edge_list):\n",
    "        f.write(\"e \" + str(e[0]) + \" \" + str(e[1]) + \" \" + str(e[2]))\n",
    "        f.write(\"\\n\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321342/321342 [00:00<00:00, 430698.95it/s]\n",
      "100%|██████████| 3327460/3327460 [00:05<00:00, 665485.05it/s]\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "id": "221c6764",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:44:27.736514Z",
     "start_time": "2025-09-02T11:44:27.075120Z"
    }
   },
   "source": [
    "# Save to ID to ID mapping for later query transformation\n",
    "with open(output_id_to_id, \"w\") as f:\n",
    "    json.dump(id_to_id_mapping, f)\n",
    "\n",
    "with open(output_id_to_id_predicate, \"w\") as f:\n",
    "    json.dump(id_to_id_mapping_predicate, f)"
   ],
   "outputs": [],
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "id": "61cfc080",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T11:39:57.151473800Z",
     "start_time": "2025-08-20T15:33:16.951386Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
